{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43e90c4e",
   "metadata": {},
   "source": [
    "# <b>Application code to convert natural language to sql query</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24db7c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83641fb0",
   "metadata": {},
   "source": [
    " **Creating a prompt template**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71489844",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are a chatbot that will interact with a human,you only have to answer to the question relating\n",
    "to queries.\n",
    "\n",
    "Given the following context and a human input,Convert the input to Sql query using the context that will\n",
    "have database schema or\n",
    "If the question cannot be answered using the information \n",
    "provided answer with \"I don't know\".\n",
    "\n",
    "{context}\n",
    "\n",
    "{chat_history}\n",
    "Human: {human_input}\n",
    "Chatbot:\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f198ad4d",
   "metadata": {},
   "source": [
    "**Creating memory and initialising LLM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d03b061b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"human_input\", \"context\"], template=template\n",
    ")\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", input_key=\"human_input\")\n",
    "chain =LLMChain(\n",
    "    llm=ChatOpenAI(openai_api_key=\"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\",temperature=0)\n",
    "    , memory=memory, prompt=prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a190318e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_string(string, encoding_name):\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    import tiktoken\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "#function to order relevant chunks\n",
    "def rank_texts(relevant_texts, query, threshold=3, max_tokens=2700):\n",
    "    from sentence_transformers import CrossEncoder\n",
    "    import numpy as np\n",
    "    # Model 1: cross-encoder/stsb-roberta-large\n",
    "    # Model 2: cross-encoder/ms-marco-MiniLM-L-12-v2\n",
    "    model_encoder = CrossEncoder(\"cross-encoder/stsb-roberta-large\")\n",
    "    relevant_context = \"\"\n",
    "    query_paras_combined = [[query, para] for para in relevant_texts]\n",
    "    similarity_scores = model_encoder.predict(query_paras_combined)\n",
    "    sim_scores_argsort = list(reversed(np.argsort(similarity_scores)))\n",
    "    for idx in sim_scores_argsort:\n",
    "#         print(\"{:.2f}\\t{}\".format(similarity_scores[idx], relevant_texts[idx]))\n",
    "\n",
    "#         print('\\n********************************************\\n\\n')\n",
    "        if threshold > 0 and num_tokens_from_string(relevant_context, \"p50k_base\") + num_tokens_from_string(\n",
    "                relevant_texts[idx], \"p50k_base\") < max_tokens:\n",
    "            relevant_context += relevant_texts[idx] + \"\\n\\n\"\n",
    "            threshold = threshold - 1\n",
    "        else:\n",
    "            break\n",
    "    return relevant_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3f4eb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to split large texts into chunks\n",
    "def split_text(text, chunk_size=200, chunk_overlap=30):\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    texts = splitter.split_text(text)\n",
    "    return texts\n",
    "\n",
    "#function to get embeddings of texts\n",
    "def get_embeddings(texts):\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    # Model 1: msmarco-distilbert-base-v4 \n",
    "    # Model 2: all-MiniLM-L6-v2\n",
    "    model_vector = SentenceTransformer('msmarco-distilbert-base-v4')\n",
    "    embeddings = model_vector.encode(texts)\n",
    "    return embeddings\n",
    "\n",
    "#function to index the embeddings\n",
    "def index_embeddings(embeddings):\n",
    "    import faiss\n",
    "    d = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(d)\n",
    "    index.add(embeddings)\n",
    "    return index\n",
    "\n",
    "#function to clean texts\n",
    "def clean_text(text):\n",
    "    cleaned_string = text.replace(\"\\n\",\"\").replace('..',\"\")\n",
    "    return cleaned_string\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89ae200d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "#function to count tokens on openai calls\n",
    "def count_tokens(context,query):\n",
    "    with get_openai_callback() as cb:\n",
    "        result=chain({\"context\":context , \"human_input\": query},return_only_outputs=True)\n",
    "        print(f'Spent a total of {cb.total_tokens} tokens')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6824bd1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provide the context(database schema) and the query in natural language, the application\n",
      "      will convert it into sql query\n",
      "Input the context(database schema)Table 1: employee      Employee_id (PK)      FirstName      LastName     JobType  Table 2: address     Address_id (PK)     AddressLine1     AddressLine2     City     Country     PostCode       Table 3: employee_address     Employee_address_id (PK)     Employee_id (FK)     Address_id (FK)      Table 4: payment     Payment_id (PK)     Form      Date      Amount     Status  Table 5: customer     Customer_id (PK)     FirstName      LastName  Table 6: customer_address     Customer_address_id (PK)     Customer_id (FK)     Address_id (FK)  Table 7: invoice     Invoice_id (PK)     Customer_id (FK)     Payment_id (FK)     Date      Status   Table 8: order      Order_id (PK)     Product_id (FK)     Quantity      Date      Status   Table 9: product     Product_id (PK)     Name      Price      Description   Table 10: supplier     Supplier_id (PK)     Name      Address_id (FK)  Table 11: inventory     Inventory_id (PK)     Product_id (FK)     Supplier_id (FK)     Quantity      Date  Table 12: shipment     Shipment_id (PK)     Order_id (FK)     ShipDate      Status   Table 13: accounts     Accounts_id (PK)     Employee_id (FK)     Balance     Date  Table 14: promotions     Promotions_id (PK)     Product_id (FK)     Customer_id (FK)     Discount     StartDate      EndDate       Table 15: feedback     Feedback_id (PK)     Product_id (FK)     Customer_id (FK)     Date     Rating      Comments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shivanksharma4/anaconda3/lib/python3.10/site-packages/torch/cuda/__init__.py:107: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input the statement in natural language write to query to return names of the customers who has not given feedbacks\n",
      "Spent a total of 286 tokens\n",
      "SELECT DISTINCT c.name \n",
      "FROM customer c \n",
      "LEFT JOIN feedback f ON c.customer_id = f.customer_id \n",
      "WHERE f.feedback_id IS NULL\n",
      "Input the statement in natural language \n",
      "Input the context(database schema)\n"
     ]
    }
   ],
   "source": [
    "print('''Provide the context(database schema) and the query in natural language, the application\n",
    "      will convert it into sql query''')\n",
    "while True:\n",
    "    context= input(\"Input the context(database schema)\")\n",
    "    if not context:\n",
    "        break\n",
    "    text_chunks = split_text(context)\n",
    "    cleaned_context = [clean_text(para) for para in text_chunks]\n",
    "    embedded_context=get_embeddings(cleaned_context)\n",
    "    \n",
    "    index = index_embeddings(embedded_context)\n",
    "\n",
    "    \n",
    "    while True:\n",
    "        query = input(\"Input the statement in natural language \")\n",
    "        if not query:\n",
    "            break\n",
    "        query_vec =get_embeddings([query])\n",
    "        k=5\n",
    "        D,I = index.search(query_vec, k)\n",
    "        relevant_indexes = I.tolist()[0]\n",
    "\n",
    "        #fetching the relevant chunks\n",
    "        relevant_chunks = []\n",
    "        for i in relevant_indexes:\n",
    "            relevant_chunks.append(text_chunks[i]) \n",
    "        context = rank_texts(relevant_chunks, query) \n",
    "        print(count_tokens(context,query)['text'])\n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b087e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: write to query to return names of the customers who has not given feedbacks\n",
      "AI: SELECT DISTINCT c.name \n",
      "FROM customer c \n",
      "LEFT JOIN feedback f ON c.customer_id = f.customer_id \n",
      "WHERE f.feedback_id IS NULL\n"
     ]
    }
   ],
   "source": [
    "print(chain.memory.buffer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
